{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8437c0e-d1d3-4977-9625-14d169ee4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import copy\n",
    "import yaml\n",
    "\n",
    "# URL = 'https://www.hlidacstatu.cz/data/Hledat/stenozaznamy-psp/'  # Replace with the target URL\n",
    "URL = 'https://www.psp.cz/eknih/2021ps/stenprot/index.htm'  # Replace with the target URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707fceee-d566-41cc-9e2d-e6589a1e731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "\n",
    "# Constants\n",
    "\n",
    "def fetch_website_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the content of the website at the given URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The target URL to fetch.\n",
    "    \n",
    "    Returns:\n",
    "        str: The raw content of the webpage.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Failed to fetch the webpage. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def extract_html_of_a_with_href(soup):\n",
    "    \"\"\"\n",
    "    Parses the required data from the given HTML content.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): The raw HTML content.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of data extracted from the content.\n",
    "    \"\"\"\n",
    "    # Modify this section to extract the specific data you need\n",
    "    # For instance, to extract all the text inside paragraph tags:\n",
    "    data = [(a.get_text(), a['href']) for a in soup.find_all('a', href=True)]\n",
    "    return data\n",
    "\n",
    "def extract_html_of_div_with_id(soup, target_id):\n",
    "    \"\"\"\n",
    "    Extracts the inner HTML content of a div with the given ID from the provided HTML content.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): The raw HTML content.\n",
    "        target_id (str): The ID of the div whose content needs to be extracted.\n",
    "    \n",
    "    Returns:\n",
    "        str: The inner HTML content of the div, or None if the div isn't found.\n",
    "    \"\"\"\n",
    "    # Find the div with the given ID\n",
    "    div = soup.find('div', id=target_id)\n",
    "    \n",
    "    # Return its inner HTML if found\n",
    "    return div if div else None\n",
    "\n",
    "def extract_content_of_p_with_align(html_content, target_align):\n",
    "    \"\"\"\n",
    "    Extracts the content of all <p> tags with the specified align attribute.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): The raw HTML content.\n",
    "        target_align (str): The align attribute value to look for.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of content extracted from the <p> tags.\n",
    "    \"\"\"\n",
    "    # Find all <p> tags with the specified align attribute\n",
    "    p_tags = html_content.find_all('p', align=target_align)\n",
    "    \n",
    "    # Extract and return the content of these tags\n",
    "    return [p.get_text() for p in p_tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5794bb-6efd-48c4-8803-5575a8a85f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_schuze_links(URL):\n",
    "    url_data = {}\n",
    "    soup = fetch_website_content(URL)\n",
    "    body_content = extract_html_of_div_with_id(soup, \"body\")\n",
    "    hrefs = extract_html_of_a_with_href(body_content)\n",
    "    links = [link for link in hrefs if \"schuz\" in link[1] and \"index\" in link[1]]\n",
    "    for link in links:\n",
    "        url_data[link[0]] = {\"url\": URL.split(\"/index\")[0] + \"/\" + link[1]}\n",
    "\n",
    "    return url_data\n",
    "\n",
    "def get_all_schuze_content(URL, URL_schuze):\n",
    "    soup = fetch_website_content(URL_schuze)\n",
    "    body_content = extract_html_of_div_with_id(soup, \"main-content\")\n",
    "    hrefs = extract_html_of_a_with_href(body_content)\n",
    "    links = [link[1] for link in hrefs if \"-\" in link[1]]\n",
    "    all_url_schuze = [URL_schuze.split(\"/index\")[0] + \"/\" + link_schuze for link_schuze in links]\n",
    "\n",
    "    return all_url_schuze\n",
    "\n",
    "def get_all_speakers(URL_schuze, URL_zaznam):\n",
    "    url_data = []\n",
    "    soup = fetch_website_content(URL_zaznam)\n",
    "    body_content = extract_html_of_div_with_id(soup, \"body\")\n",
    "    hrefs = extract_html_of_a_with_href(body_content)\n",
    "    links = [link for link in hrefs if \"#r\" in link[1]]\n",
    "    for link in links:\n",
    "        url_data.append((link[0], URL_schuze.split(\"/index\")[0] + \"/\" + link[1]))\n",
    "\n",
    "    return url_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5d1cce-15e2-41aa-99fa-6a86afc6d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(URL):    \n",
    "    schuze_all = get_all_schuze_links(URL)\n",
    "    for name_schuze in list(schuze_all.keys()):\n",
    "        URL_schuze = schuze_all[name_schuze][\"url\"]\n",
    "        all_url_schuze_parts = get_all_schuze_content(URL, URL_schuze)\n",
    "        schuze_all[name_schuze][\"rec\"] = all_url_schuze_parts\n",
    "        schuze_all[name_schuze][\"speak\"] = []\n",
    "        for URL_zaznam in all_url_schuze_parts[0:1]:\n",
    "            schuze_all[name_schuze][\"speak\"] += get_all_speakers(URL_schuze, URL_zaznam)\n",
    "            \n",
    "    return schuze_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2405172-b392-4625-8bad-b51be86879f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(new_data, new_key, file_name):\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    \n",
    "    data[new_key] = new_data\n",
    "    \n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        yaml.dump(data, file, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8299e764-f490-40c6-9041-a27caa7457f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data(schuze_all, file_name):\n",
    "    all_data = {}\n",
    "    for name_schuze in list(schuze_all.keys()):\n",
    "        schuze_data = {}\n",
    "        for link in schuze_all[name_schuze][\"speak\"]:\n",
    "            schuze_data[link[0]] = []\n",
    "            try:\n",
    "                soup = fetch_website_content(link[1])\n",
    "                body_content = extract_html_of_div_with_id(soup, \"main-content\")\n",
    "                p_tags = extract_content_of_p_with_align(body_content, \"justify\")\n",
    "\n",
    "                schuze_data[link[0]] += [tag.replace(\"\\xa0\", \" \").replace(\"§\", \"zk.\").replace(\"*\", \"\") for tag in p_tags]\n",
    "            except:\n",
    "                print(\"Couldnt get data from: \", link[1])\n",
    "        try:\n",
    "            save_data(schuze_data, name_schuze, file_name)\n",
    "        except:\n",
    "            print(\"Couldnt save data from: \", name_schuze)\n",
    "            print(\"\\n\")\n",
    "        all_data[name_schuze] = copy.deepcopy(schuze_data)\n",
    "        print(\"Done downloading data from: \", name_schuze)\n",
    "        print(\"\\n\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c47a3ca4-577a-4ac0-87a5-454a6be82c0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_links = get_all_links(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea5fad9-4574-4060-9a88-fe1081d634e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1. schůze', '2. schůze', '3. schůze', '4. schůze', '5. schůze', '6. schůze', '7. schůze', '8. schůze', '9. schůze', '10. schůze', '11. schůze', '12. schůze', '13. schůze', '14. schůze', '15. schůze', '16. schůze', '17. schůze', '18. schůze', '19. schůze', '20. schůze', '21. schůze', '22. schůze', '23. schůze', '24. schůze', '25. schůze', '26. schůze', '27. schůze', '28. schůze', '29. schůze', '30. schůze', '31. schůze', '32. schůze', '33. schůze', '34. schůze', '35. schůze', '36. schůze', '37. schůze', '38. schůze', '39. schůze', '40. schůze', '41. schůze', '42. schůze', '43. schůze', '44. schůze', '45. schůze', '46. schůze', '47. schůze', '48. schůze', '49. schůze', '50. schůze', '51. schůze', '52. schůze', '53. schůze', '54. schůze', '55. schůze', '56. schůze', '57. schůze', '58. schůze', '59. schůze', '60. schůze', '61. schůze', '62. schůze', '63. schůze', '64. schůze', '65. schůze', '66. schůze', '67. schůze', '68. schůze', '69. schůze', '70. schůze', '71. schůze', '72. schůze', '73. schůze', '74. schůze', '75. schůze', '76. schůze', '77. schůze', '78. schůze', '79. schůze', '80. schůze'])\n"
     ]
    }
   ],
   "source": [
    "print(all_links.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e4559e-fc08-45d9-a35e-ac187438d0eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done downloading data from:  1. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  2. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  3. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  4. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  5. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  6. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  7. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  8. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  9. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  10. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  11. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  12. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  13. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  14. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  15. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  16. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  17. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  18. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  19. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  20. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  21. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  22. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  23. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  24. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  25. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  26. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  27. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  28. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  29. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  30. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  31. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  32. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  33. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  34. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  35. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  36. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  37. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  38. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  39. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  40. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  41. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  42. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  43. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  44. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  45. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  46. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  47. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  48. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  49. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  50. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  51. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  52. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  53. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  54. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  55. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  56. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  57. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  58. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  59. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  60. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  61. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  62. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  63. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  64. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  65. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  66. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  67. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  68. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  69. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  70. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  71. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  72. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  73. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  74. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  75. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  76. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  77. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  78. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  79. schůze\n",
      "\n",
      "\n",
      "Done downloading data from:  80. schůze\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data = get_all_data(all_links, \"./data/ps_scrappe.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d86280ea-3300-4079-9f76-4537fdfddc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bec9cb50-2981-4c61-82f0-0378705bf3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yaml\n",
    "\n",
    "# # Save dictionary to a YAML file with special characters preserved\n",
    "# with open(\"./data/ps_scrappe.yaml\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     yaml.dump({}, file, allow_unicode=True)\n",
    "\n",
    "# print(\"Data saved to ps_scrappe.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
