{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2f718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryMemory, ConversationSummaryBufferMemory, ConversationKGMemory\n",
    "import os\n",
    "# import torch\n",
    "from transformers import AutoTokenizer, pipeline, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e139ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/martin/openaiKey.txt', 'r') as file:\n",
    "    key_content = file.read()\n",
    "\n",
    "OPENAI_API_KEY = str(key_content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5696763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('contexts_embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "    \n",
    "with open('contexts.pkl', 'rb') as f:\n",
    "    contexts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7908da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "feature_extraction_pipeline = pipeline('feature-extraction', model=model, tokenizer=tokenizer)  # 'device=0' for GPU, remove for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd18d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingProcessor:\n",
    "    def __init__(self, pipeline=feature_extraction_pipeline, chunk_size=200):\n",
    "        \"\"\"\n",
    "        Initializes the EmbeddingProcessor.\n",
    "\n",
    "        Parameters:\n",
    "        - pipeline (optional): The feature extraction pipeline to be used for embedding. Default is feature_extraction_pipeline.\n",
    "        - chunk_size (optional): The size of the text chunks for processing. Default is 200.\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        nltk.download('stopwords')\n",
    "        self.stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "    def _preprocess_text(self, sentence):\n",
    "        \"\"\"\n",
    "        Preprocesses a given sentence by removing stop words.\n",
    "\n",
    "        Parameters:\n",
    "        - sentence: The input sentence to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "        - str: The preprocessed sentence.\n",
    "        \"\"\"\n",
    "        filtered_tokens = [word for word in sentence.split() if word.lower() not in self.stop_words_en]\n",
    "        return ' '.join(filtered_tokens)\n",
    "\n",
    "    def get_embedding(self, prompt, preprocess=True):\n",
    "        \"\"\"\n",
    "        Retrieves the embedding for a given prompt.\n",
    "\n",
    "        Parameters:\n",
    "        - prompt: The input text prompt.\n",
    "        - preprocess (optional): If True, preprocesses the prompt by removing stop words. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The embedding for the prompt.\n",
    "        \"\"\"\n",
    "        if preprocess:\n",
    "            prompt = self._preprocess_text(prompt)\n",
    "        chunks = [prompt[i:i + self.chunk_size] for i in range(0, len(prompt), self.chunk_size)]\n",
    "        chunk_embeddings = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_embedding = self.pipeline(chunk)\n",
    "            chunk_embedding = np.mean(chunk_embedding[0], axis=0)\n",
    "            chunk_embeddings.append(chunk_embedding)\n",
    "\n",
    "        embedding = np.mean(chunk_embeddings, axis=0).reshape(1, -1)\n",
    "        return embedding\n",
    "\n",
    "\n",
    "class PromptProcessor(EmbeddingProcessor):\n",
    "    def __init__(self, pipeline=feature_extraction_pipeline, source_lang='cs', chunk_size=200):\n",
    "        \"\"\"\n",
    "        Initializes the PromptProcessor.\n",
    "\n",
    "        Parameters:\n",
    "        - pipeline (optional): The feature extraction pipeline to be used for embedding. Default is feature_extraction_pipeline.\n",
    "        - source_lang (optional): The source language for translation. Default is 'cs'.\n",
    "        - chunk_size (optional): The size of the text chunks for processing. Default is 200.\n",
    "        \"\"\"\n",
    "        super().__init__(pipeline=pipeline, chunk_size=chunk_size)\n",
    "        self.translator = GoogleTranslator()\n",
    "        self.source_lang = source_lang\n",
    "        self.last_prompt_class = None\n",
    "    \n",
    "    def _translate(self, prompt):\n",
    "        \"\"\"\n",
    "        Translates the given prompt from the source language to the target language.\n",
    "\n",
    "        Parameters:\n",
    "        - prompt: The input text prompt.\n",
    "\n",
    "        Returns:\n",
    "        - str: The translated prompt.\n",
    "        \"\"\"\n",
    "        return self.translator.translate(prompt, src=self.source_lang, dest=\"en\")\n",
    "\n",
    "    def _get_prompt_embedding_class(self, prompt_embedding, embeddings, threshold=0.0, difference_threshold=0.0):\n",
    "        \"\"\"\n",
    "        Determines the class of the prompt based on the similarity with pre-defined embeddings.\n",
    "\n",
    "        Parameters:\n",
    "        - prompt_embedding: The embedding of the prompt.\n",
    "        - embeddings: Dictionary containing pre-defined embeddings.\n",
    "        - threshold (optional): The similarity threshold. If no similarity exceeds the threshold, return None. Default is 0.0.\n",
    "\n",
    "        Returns:\n",
    "        - str or None: The class of the prompt. Returns None if no similarity exceeds the threshold.\n",
    "        \"\"\"\n",
    "        prompt_class = None\n",
    "        max_sim = -1\n",
    "        second_max_sim = -1\n",
    "        for emb_name, emb_t in embeddings.items():\n",
    "            sim = cosine_similarity(prompt_embedding, emb_t)\n",
    "            if sim > max_sim:\n",
    "                prompt_class = emb_name\n",
    "                second_max_sim = max_sim\n",
    "                max_sim = sim\n",
    "                \n",
    "        if abs(max_sim - second_max_sim) < difference_threshold:\n",
    "            return None\n",
    "        # Check if the maximum similarity exceeds the threshold\n",
    "        if max_sim <= threshold:\n",
    "            return None\n",
    "        \n",
    "\n",
    "        return prompt_class\n",
    "\n",
    "    \n",
    "    def process_prompt(self, prompt, embeddings, preprocess=True, translate=True, threshold=0.0, difference_threshold=0.0):\n",
    "        \"\"\"\n",
    "        Processes a prompt by translating, preprocessing, and obtaining its embedding class.\n",
    "\n",
    "        Parameters:\n",
    "        - prompt: The input text prompt.\n",
    "        - embeddings: Dictionary containing pre-defined embeddings.\n",
    "        - preprocess (optional): If True, preprocesses the prompt by removing stop words. Default is True.\n",
    "        - translate (optional): If True, translates the prompt. Default is True.\n",
    "        - threshold (optional): The similarity threshold. If no similarity exceeds the threshold, return None. Default is 0.0.\n",
    "\n",
    "        Returns:\n",
    "        - str or None: The class of the prompt. Returns None if no similarity exceeds the threshold.\n",
    "        \"\"\"\n",
    "        if translate:\n",
    "            translated_prompt = self._translate(prompt)\n",
    "        prompt_embedding = self.get_embedding(translated_prompt, preprocess)\n",
    "        prompt_class = self._get_prompt_embedding_class(prompt_embedding, embeddings, threshold, difference_threshold)\n",
    "        \n",
    "        return prompt_class\n",
    "    \n",
    "    def format_prompt(self, prompt, embeddings, preprocess=True, translate=True, threshold=0.0, difference_threshold=0.0):\n",
    "        \"\"\"\n",
    "        Formats a prompt by translating, preprocessing, obtaining its embedding class, and adding context information.\n",
    "\n",
    "        Parameters:\n",
    "        - prompt: The input text prompt.\n",
    "        - embeddings: Dictionary containing pre-defined embeddings.\n",
    "        - preprocess (optional): If True, preprocesses the prompt by removing stop words. Default is True.\n",
    "        - translate (optional): If True, translates the prompt. Default is True.\n",
    "        - threshold (optional): The similarity threshold. If no similarity exceeds the threshold, return None. Default is 0.0.\n",
    "\n",
    "        Returns:\n",
    "        - str or None: The formatted prompt including context information. Returns None if no similarity exceeds the threshold.\n",
    "        \"\"\"\n",
    "        prompt_class = self.process_prompt(prompt, embeddings, preprocess, translate, threshold, difference_threshold)\n",
    "        \n",
    "        # Check if prompt_class is None, and handle accordingly\n",
    "        if prompt_class is None:\n",
    "            return f\"{prompt} Kontext: {contexts[last_prompt_class][0]}\"\n",
    "        else:\n",
    "            self.last_prompt_class = prompt_class\n",
    "            return formatted_prompt = f\"{prompt} Kontext: {contexts[prompt_class][0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b3da4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trenovat bez kontextu je o dost horsi, cestina porad obcas random\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model_name='ft:gpt-3.5-turbo-1106:personal::8V1zpYfy', openai_api_key = OPENAI_API_KEY)\n",
    "memory_llm = ChatOpenAI(temperature=0.9, model_name='gpt-3.5-turbo-1106', openai_api_key = OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d7da819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "#template of a prompt\n",
    "template = \"\"\"Jsi premiér České republiky Petr Fiala. Odpověz krátce jako Petr Fiala na následující prompt: {input}.\n",
    "Shrnutí předchozí konverzace:\n",
    "{history}\n",
    "Pokud si nejsi jistý, tak odmítni odpovědět.\n",
    "Petr Fiala:\"\"\"\n",
    "\n",
    "context=[]\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationSummaryMemory(llm=memory_llm, ai_prexis= \"Petr Fiala\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f587216",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Jsi premiér České republiky Petr Fiala. Odpověz jako Petr Fiala na následující prompt: FDJFDSJFSD\n",
    "\n",
    "Petr Fiala: 'Jsem prozápadní politik a proevropský politik, ale EU je potřeba reformovat. Jsem kritický vůči regulacím, Green dealu a migrační politice. Měli bychom přijímat méně migrantů z arabských zemí a odmítám kvóty na přerozdělování imigrantů. Je důležité, aby EU jednala už v zemích, odkud se emigruje. Jako země závislá na exportu musíme v EU zůstat. Moje koalice SPOLU kandiduje do Europarlamentu a bude prosazovat zájmy České republiky.'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c85ad8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/martin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "prompt_processer = PromptProcessor(pipeline=feature_extraction_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23985c24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mJsi premiér České republiky Petr Fiala. Odpověz krátce jako Petr Fiala na následující prompt: Co si myslíš o EU? Kontext: Petr Fiala se označuje za prozápadního politika a proevropského politika. K Evropské unii je často však kritický, zejména co se týče regulací a nařízení, Green dealu nebo migrační politiky. EU je podle Petra Fialy potřeba reformovat. Jako země závislá na exportu musíme v EU zůstat. Měli bychom přijímat méně migrantů z arabských zemí. Důrazně odmítá kvóty na přerozdělování imigrantů. Petr Fiala tvrdí, že je důležité, aby Evropská unie jednala už v zemích, odkud se emigruje. Aktuální prezidentka Evropské unie je Ursula von der Leyen. Čekají nás volby do Europarlamentu. Petr Fiala je součástí koalice SPOLU, která do europarlamentu kandiduje. Kandidátní listinu vede Alexandr Vondra, na druhém místě je Veronika Vrecionová a na třetím místě Luděk Niedermayer. Petr Fiala tvrdí, že je koalice SPOLU silná a bude v Evropském parlamentu bude prosazovat zájmy České republiky..\n",
      "Shrnutí předchozí konverzace:\n",
      "\n",
      "Petr Fiala:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jsem prozápadní politik a proevropský politik, ale EU je potřeba reformovat. Jsem kritický vůči regulacím, Green dealu a migrační politice. Měli bychom přijímat méně migrantů z arabských zemí a odmítám kvóty na přerozdělování imigrantů. Je důležité, aby EU jednala už v zemích, odkud se emigruje. Jako země závislá na exportu musíme v EU zůstat. Moje koalice SPOLU kandiduje do Europarlamentu a bude prosazovat zájmy České republiky.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt = prompt_processer.format_prompt(\"Co si myslíš o EU?\", embeddings, threshold = 0.2, difference_threshold=0.05)\n",
    "conversation.predict(input=formatted_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
