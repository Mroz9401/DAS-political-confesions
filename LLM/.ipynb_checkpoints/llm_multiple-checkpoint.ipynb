{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f1bf57-c506-4ed7-a2a6-59d5e0721d85",
   "metadata": {},
   "source": [
    "# LLM for multiple people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d8413-1650-4d54-8f72-68eebefa619d",
   "metadata": {},
   "source": [
    "## Data Preparation:\r\n",
    "Your dataset is structured in a sequential manner with people responding to prior statements. First, ensure the data is organized in pairs where you have an input statement and the response to that statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f32608d-59b9-4a1c-877d-1f8253311d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Data Structure\n",
    "data = [\n",
    "    {\"input\": \"How are you?\", \"response\": \"I'm good, thanks.\"},\n",
    "    {\"input\": \"What's your favorite color?\", \"response\": \"Blue.\"},\n",
    "    {\"input\": \"What do you do for a living?\", \"response\": \"I am politician.\"},\n",
    "    {\"input\": \"What is your pla for the weekend?\", \"response\": \"I plan to go to the mountains with my family this weekend.\"},\n",
    "    {\"input\": \"What do you like best about your job?\", \"response\": \"Conversations with people.\"},\n",
    "    {\"input\": \"How often do you play sports?\", \"response\": \"Only once a week.\"},\n",
    "    {\"input\": \"What is your favourite sport?\", \"response\": \"My favourite sport is football.\"},\n",
    "    {\"input\": \"What's your favorite food?\", \"response\": \"My favorite is fried chicken from KFC.\"},\n",
    "    # ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ec047-c1c8-485b-b0cf-65223017f8e3",
   "metadata": {},
   "source": [
    "## Data Split:\r\n",
    "Split the data into training, validation, and test sets. This ensures that your model can generalize well and is not overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07fdccaa-bca8-4808-a81b-687cca0ca1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "import accelerate\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train, temp = train_test_split(data, test_size=0.2, random_state=42)\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6dd179b-d7e1-490d-8b5e-91624402dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    for entry in train:\n",
    "        file.write(entry['input'] + '\\n')\n",
    "        file.write(entry['response'] + '\\n')\n",
    "\n",
    "with open('./data/valid.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    for entry in train:\n",
    "        file.write(entry['input'] + '\\n')\n",
    "        file.write(entry['response'] + '\\n')\n",
    "        \n",
    "# with open('./data/train.txt', 'w') as file:\n",
    "#     for entry in train:\n",
    "#         file.write(f\"{entry['input']} -> {entry['response']}\\n\")\n",
    "\n",
    "# with open('./data/valid.txt', 'w') as file:\n",
    "#     for entry in valid:\n",
    "#         file.write(f\"{entry['input']} -> {entry['response']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c4b4320-6c63-49bf-b9fa-e17d79f30c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.0\n"
     ]
    }
   ],
   "source": [
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96903bdf-6e24-48d4-ba5a-243f3e5c00c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e960b99-44d4-49be-9fee-877f53798e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.training_args.is_accelerate_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9e13fe-88ef-4004-922f-1da3050b342d",
   "metadata": {},
   "source": [
    "## Training:\r\n",
    "You'll need a suitable framework for training large language models. While OpenAI's own models are not open source, architectures like GPT can be trained using HuggingFace's Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd365435-a875-4700-bfa8-0787efce814b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiri.nabelek\\FJFI\\DAS-Political_confessions\\env_llm\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "C:\\Users\\jiri.nabelek\\FJFI\\DAS-Political_confessions\\env_llm\\lib\\site-packages\\transformers\\training_args.py:1270: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ğŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 43\u001b[0m\n\u001b[0;32m     34\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME)\n\u001b[0;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     36\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     37\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mvalid_dataloader,\n\u001b[0;32m     41\u001b[0m )\n\u001b[1;32m---> 43\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\FJFI\\DAS-Political_confessions\\env_llm\\lib\\site-packages\\transformers\\trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\FJFI\\DAS-Political_confessions\\env_llm\\lib\\site-packages\\transformers\\trainer.py:1870\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1867\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1870\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1871\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32m~\\FJFI\\DAS-Political_confessions\\env_llm\\lib\\site-packages\\accelerate\\data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32m~\\FJFI\\DAS-Political_confessions\\env_llm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\FJFI\\DAS-Political_confessions\\env_llm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\FJFI\\DAS-Political_confessions\\env_llm\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\FJFI\\DAS-Political_confessions\\env_llm\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2-medium\"  # Or another suitable variant\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the data\n",
    "# train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"./data/train.txt\", block_size=128)\n",
    "# valid_dataset = TextDataset(tokenizer=tokenizer, file_path=\"./data/valid.txt\", block_size=128)\n",
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"./data/train.txt\", block_size=128)\n",
    "valid_dataset = TextDataset(tokenizer=tokenizer, file_path=\"./data/valid.txt\", block_size=128)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train,  # The training samples.\n",
    "            sampler = RandomSampler(train), # Select batches randomly\n",
    "            batch_size = 32 # Trains with this batch size.\n",
    "        )\n",
    "valid_dataloader = DataLoader(\n",
    "            valid,  # The training samples.\n",
    "            sampler = RandomSampler(valid), # Select batches randomly\n",
    "            batch_size = 32 # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments and initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,  # Increase this for real training\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "trainer = Trainer(\n",
    "    model=model.to('cpu'),\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataloader,\n",
    "    eval_dataset=valid_dataloader,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bad8a33-b2ef-48c8-b2f9-0f0432a83e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2437, 389, 345, 30]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.encode(train[0][\"input\"])\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13492039-8108-4bd4-add0-4c00c564ac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"./data/train.txt\", block_size=128)\n",
    "print(train_dataset.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "695ce458-1ca2-4baf-91b5-445ed08a704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "            train,  # The training samples.\n",
    "            sampler = RandomSampler(train), # Select batches randomly\n",
    "            batch_size = 32 # Trains with this batch size.\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd86ecf1-8400-44dd-8d47-3ba7f3fcf720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000002042D8A6FB0>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24998d5-cf0c-41fb-8bdc-15c4ed16e589",
   "metadata": {},
   "source": [
    "## Model Evaluation:\r\n",
    "After training, evaluate the models on the test dataset to gauge their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b82a084-cdd4-4dd8-b9e6-0f63361de818",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "\n",
    "print(f\"Perplexity: {math.exp(results['eval_loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8871d2-7f88-433d-b209-1d20e94a75c7",
   "metadata": {},
   "source": [
    "## Fine-tuning & Iteration:\r\n",
    "The first model might not perfectly mimic your target person. You might need to:\r\n",
    "\r\n",
    "Collect more data.\r\n",
    "Modify the architecture or training parameters.\r\n",
    "Use additional techniques like transfer learning or attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6185bc-a7fb-4fc4-8841-fdefcfba2438",
   "metadata": {},
   "source": [
    "## Incorporating the Interaction:\r\n",
    "The model you've trained understands the structure of a conversation but to make it behave like it's responding to a statement, simply provide the input statement as a prompt and let the model generate a response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
